# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1njfnOPnNqp7gwUMXM04OHP7pBTorTj-f
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('./HeartDiseaseTrain-Test.csv')

df

df.describe()
df.info()
df.head()

print(df.isnull().sum())

df = df.rename(columns={'sex': 'is_male'})
df['is_male'] = df['is_male'].apply(lambda x: 1 if x == 'Male' else 0)
display(df.head())

df['exercise_induced_angina'] = df['exercise_induced_angina'].apply(lambda x: 1 if x == 'Yes' else 0)
display(df.head())

df['fasting_blood_sugar'] = df['fasting_blood_sugar'].apply(lambda x: 1 if x == 'Greater than 120 mg/ml' else 0)
display(df.head())

df = df.drop('chest_pain_type', axis=1)
display(df.head())

from sklearn.preprocessing import LabelEncoder

categorical_cols = df.select_dtypes(include='object').columns
label_encoders = {}
for col in categorical_cols:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

display(df.head())

correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix[['target']], annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation with Target Variable')
plt.show()

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()



multicollinearity_oldpeak_slope = df['oldpeak'].corr(df['slope'])
print(f"Multicollinearity (correlation) between oldpeak and slope: {multicollinearity_oldpeak_slope:.2f}")

# The correlation between target and Max_heart_rate is already shown in the heatmap (0.42).
# We are interested in the correlation of features with the target, not multicollinearity involving the target itself.

from sklearn.preprocessing import StandardScaler

# Separate features (X) from the target variable (y)
X = df.drop('target', axis=1)
y = df['target']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the features
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame (optional, but good for readability)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

display(X_scaled.head())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize the Logistic Regression model
model = LogisticRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

"""Accuracy (0.77): This tells you the overall percentage of correct predictions your model made. An accuracy of 0.77 means the model was correct 77% of the time. While useful, it can be misleading if your data is imbalanced (more cases of one outcome than the other).
Confusion Matrix: This table shows you exactly where your model's predictions were right and wrong:
True Positives (TP): The model correctly predicted that a patient has heart disease (84).
True Negatives (TN): The model correctly predicted that a patient does not have heart disease (74).
False Positives (FP): The model incorrectly predicted that a patient has heart disease when they don't (28). This is also known as a Type I error.
False Negatives (FN): The model incorrectly predicted that a patient does not have heart disease when they actually do (19). This is also known as a Type II error.
Precision (for class 1, 0.75): When your model predicts a patient has heart disease (class 1), it is correct 75% of the time. The formula is TP / (TP + FP).
Recall (for class 1, 0.82): Of all the patients who actually have heart disease (class 1), your model correctly identified 82% of them. The formula is TP / (TP + FN). Recall is also known as Sensitivity.
F1-score (for class 1, 0.78): This is a balance between Precision and Recall. It's a good metric when you want to consider both false positives and false negatives. The formula is 2 * (Precision * Recall) / (Precision + Recall).
ROC Curve and AUC Score (0.87): The ROC curve shows the trade-off between the True Positive Rate (Recall) and the False Positive Rate at different threshold settings. The AUC (Area Under the Curve) is a single number that summarizes the overall performance of the classifier across all possible thresholds. An AUC of 0.87 is quite good, indicating the model has a high ability to distinguish between the two classes.
Why False Negatives are a Disaster and Which Metric to Prioritize:

You are absolutely right! In the context of heart disease prediction, a false negative is much more critical than a false positive.

False Negative (predicting no heart disease when there is): This is disastrous because a patient with heart disease would not receive the necessary treatment, which could have severe consequences.
False Positive (predicting heart disease when there isn't): While not ideal (it can cause unnecessary anxiety and further testing), it's generally less harmful than a false negative.
To minimize false negatives, the most important metric to prioritize is Recall (Sensitivity) for the positive class (heart disease).



Prioritizing Recall:

A high Recall means your model is good at identifying most of the actual positive cases (patients with heart disease). While increasing Recall might lead to an increase in False Positives, in this scenario, it's often a necessary trade-off to ensure that as few patients with heart disease as possible are missed.



When evaluating models for heart disease prediction, you would typically focus on maximizing Recall, perhaps even at the expense of some precision. The specific balance you strike would depend on the clinical implications and acceptable levels of false positives and false negatives in a real-world setting.

**Observation on Model Evaluation:**

In the context of heart disease prediction, minimizing **False Negatives** (predicting no heart disease when it is present) is crucial, as missing a diagnosis can have severe consequences. Therefore, **Recall (Sensitivity)** for the positive class (heart disease) is a critical metric to prioritize during model evaluation and selection, even if it means accepting a slightly higher number of False Positives.
"""