# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r-2LoNgFosILN_FKyGOtIxTYxsmOyNm1
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy matplotlib seaborn scikit-learn

import pandas as pd

df_titanic = pd.read_csv('titanic_train.csv')
display(df_titanic.head())

df_titanic['Age_imputed'] = df_titanic.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))
display(df_titanic[['Age', 'Age_imputed', 'Pclass', 'Sex']].head())

df_titanic['Cabin'] = df_titanic['Cabin'].fillna('Unknown')

df_titanic['CabinDeck'] = df_titanic['Cabin'].apply(lambda x: x[0] if pd.notnull(x) and x != 'Unknown' else 'Unknown')

# Show the counts for each CabinDeck
print("Counts for each CabinDeck:")
display(df_titanic['CabinDeck'].value_counts())

# Show survival rate by CabinDeck
print("Survival rate by CabinDeck:")
display(df_titanic.groupby('CabinDeck')['Survived'].mean() * 100)

df_titanic['is_male'] = df_titanic['Sex'].map({'male': 1, 'female': 0})
display(df_titanic[['Sex', 'is_male']].head())

df_titanic = df_titanic.drop('Name', axis=1)
display(df_titanic.head())

df_titanic = df_titanic.drop('Ticket', axis=1)
display(df_titanic.head())

# One-hot encode 'Embarked' and 'CabinDeck'
df_titanic = pd.get_dummies(df_titanic, columns=['Embarked', 'CabinDeck'], drop_first=True)

display(df_titanic.head())

# Convert boolean columns to integers (0s and 1s)
boolean_cols = df_titanic.select_dtypes(include='bool').columns
df_titanic[boolean_cols] = df_titanic[boolean_cols].astype(int)

display(df_titanic.head())

# Identify numerical features (excluding the target variable 'Survived' and the index 'PassengerId')
numerical_features = df_titanic.select_dtypes(include=['int64', 'float64']).columns.tolist()
features_to_scale = [feature for feature in numerical_features if feature not in ['Survived', 'PassengerId']]

print("Numerical features to scale:")
display(features_to_scale)

from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Apply the scaler to the numerical features
df_titanic[features_to_scale] = scaler.fit_transform(df_titanic[features_to_scale])

display(df_titanic.head())

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
# Drop the original 'Age' column and 'Sex' since we have 'Age_imputed' and 'is_male'
# Drop 'Cabin' as it was handled by 'CabinDeck'
X = df_titanic.drop(['Survived', 'Age', 'Sex', 'Cabin'], axis=1)
y = df_titanic['Survived']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8, 10],
    'min_samples_split': [2, 5, 10]
}

# Initialize the Random Forest Classifier
rf = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Perform hyperparameter tuning
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_rf_model = grid_search.best_estimator_

from sklearn.model_selection import cross_val_score

# Evaluate the best model using cross-validation on the entire dataset
cv_scores = cross_val_score(best_rf_model, X, y, cv=5, scoring='accuracy')

print("Cross-validation scores:", cv_scores)
print("Mean cross-validation accuracy:", cv_scores.mean())