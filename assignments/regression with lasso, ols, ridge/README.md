# Regression Model Comparison â€“ Ames Housing Dataset

This project compares **Ordinary Least Squares (OLS)**, **Lasso Regression**, and **Ridge Regression** on the Ames Housing dataset.  
The main goal was to evaluate their performance, understand the effect of regularization, and compare feature importance.

---

## ğŸ“Š Model Performance

| Model  | RMSE       | MAE       | RÂ²      | Notes |
|--------|-----------:|----------:|--------:|-------|
| **OLS (Linear Regression)**  | 360,967 | 270,427 | -17.03 | ğŸš¨ Performed very poorly, worse than predicting the mean. Highly sensitive to outliers and multicollinearity. |
| **Lasso Regression (L1)**    | 66,760  | 53,347  | 0.38   | ğŸ‘ Better than OLS. Performs feature selection by shrinking some coefficients to **0**, but dropped too many useful features â†’ limited performance. |
| **Ridge Regression (L2)**    | 38,281  | 30,920  | 0.80   | ğŸ† Best performer. Keeps all features but shrinks them proportionally, leading to strong generalization and robustness against outliers. |

---

## âš–ï¸ Key Takeaways
1. **OLS** failed because it included every feature, making it very sensitive to noise.  
2. **Lasso** helped by performing automatic feature selection, but over-shrank important predictors.  
3. **Ridge** struck the best balance â€” keeping all features, but reducing overfitting via coefficient shrinkage.  

---

## ğŸ“Œ Regularization Intuition
- **L1 (Lasso)**: Some coefficients â†’ 0 (feature selection).  
- **L2 (Ridge)**: All coefficients reduced but none are eliminated.  
- **OLS**: No penalty, fits data exactly (prone to overfitting).  

---

## ğŸ”® Next Steps
- Try **ElasticNet (L1 + L2 mix)** for a balance between Ridge & Lasso.  
- Perform **feature importance analysis** (compare coefficients).  
- Add **visualizations**:  
  - Actual vs Predicted plots  
  - Error distribution  
  - Coefficient shrinkage comparison  

---
