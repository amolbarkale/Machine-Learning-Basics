# -*- coding: utf-8 -*-
"""Decision Tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ETNoUxo1DFKj4J-H0N2ZHG_TKL2czfWQ
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy matplotlib seaborn

import pandas as pd

df = pd.read_csv('/content/winequalityN.csv')
display(df.head())



df.dropna(inplace=True)
print("Total number of missing values after dropping rows:")
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['type'] = label_encoder.fit_transform(df['type'])

print("DataFrame after Label Encoding:")
display(df.head(40))

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Define features (X) and target (y)
X = df.drop('quality', axis=1)
y = df['quality']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data split into training and testing sets.")
print("Training set shape (X_train, y_train):", X_train.shape, y_train.shape)
print("Testing set shape (X_test, y_test):", X_test.shape, y_test.shape)

# Experiment with different hyperparameters

# Trial 1: Default parameters
dt_default = DecisionTreeClassifier(random_state=42)
dt_default.fit(X_train, y_train)
y_pred_default = dt_default.predict(X_test)
accuracy_default = accuracy_score(y_test, y_pred_default)
print(f"Trial 1 (Default Parameters) Accuracy: {accuracy_default:.4f}")

# Trial 2: criterion='entropy'
dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_entropy.fit(X_train, y_train)
y_pred_entropy = dt_entropy.predict(X_test)
accuracy_entropy = accuracy_score(y_test, y_pred_entropy)
print(f"Trial 2 (Criterion='entropy') Accuracy: {accuracy_entropy:.4f}")

# Trial 3: max_depth=10
dt_max_depth = DecisionTreeClassifier(max_depth=15, random_state=42)
dt_max_depth.fit(X_train, y_train)
y_pred_max_depth = dt_max_depth.predict(X_test)
accuracy_max_depth = accuracy_score(y_test, y_pred_max_depth)
print(f"Trial 3 (max_depth=10) Accuracy: {accuracy_max_depth:.4f}")

# Trial 4: min_samples_split=20
dt_min_samples_split = DecisionTreeClassifier(min_samples_split=10, random_state=42)
dt_min_samples_split.fit(X_train, y_train)
y_pred_min_samples_split = dt_min_samples_split.predict(X_test)
accuracy_min_samples_split = accuracy_score(y_test, y_pred_min_samples_split)
print(f"Trial 4 (min_samples_split=20) Accuracy: {accuracy_min_samples_split:.4f}")

# Trial 5: min_samples_leaf=10
dt_min_samples_leaf = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)
dt_min_samples_leaf.fit(X_train, y_train)
y_pred_min_samples_leaf = dt_min_samples_leaf.predict(X_test)
accuracy_min_samples_leaf = accuracy_score(y_test, y_pred_min_samples_leaf)
print(f"Trial 5 (min_samples_leaf=10) Accuracy: {accuracy_min_samples_leaf:.4f}")

# You can add more trials with combinations of hyperparameters

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# Evaluate the default Decision Tree model (dt_default)

# Predictions on the test set
y_pred_default = dt_default.predict(X_test)

# Accuracy (already calculated, but including for completeness)
accuracy_default = accuracy_score(y_test, y_pred_default)
print(f"Accuracy: {accuracy_default:.4f}")

# Precision, Recall, F1-score
print("\nClassification Report:")
print(classification_report(y_test, y_pred_default))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_default))

# ROC-AUC curve (for binary classification, need to adapt for multi-class)
# Since this is a multi-class problem, ROC-AUC is not directly applicable
# for a single curve. We can calculate it for each class against the rest (OvR)
# or micro/macro averages. Let's skip for now or calculate OvR if needed later.

# Visualize the decision tree (plotting a large tree can be resource intensive)
# Let's plot a smaller version or a specific branch if the tree is too large.
# For simplicity, let's plot a limited depth tree for visualization.
plt.figure(figsize=(20,10))
plot_tree(dt_default, filled=True, feature_names=X.columns, class_names=[str(c) for c in dt_default.classes_], max_depth=3)
plt.title("Decision Tree Visualization (max_depth=3)")
plt.show()

# Feature Importance Scores
print("\nFeature Importances:")
feature_importances = pd.Series(dt_default.feature_importances_, index=X.columns).sort_values(ascending=False)
print(feature_importances)



from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Define the hyperparameters to experiment with
hyperparameters = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
}

best_accuracy = 0
best_params = {}

# Implement a greedy approach to find the best hyperparameters
# This is a simplified greedy approach, not a true grid search
# A true grid search would test all combinations. This approach
# will iterate through each hyperparameter individually and keep
# the best value found for that hyperparameter before moving to the next.

print("Starting greedy hyperparameter tuning...")

# Start with default parameters
current_params = {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}

# Evaluate initial performance with default parameters
dt_initial = DecisionTreeClassifier(random_state=42, **current_params)
scores = cross_val_score(dt_initial, X_train, y_train, cv=5, scoring='accuracy')
current_accuracy = np.mean(scores)
print(f"Initial accuracy with default parameters: {current_accuracy:.4f}")
best_accuracy = current_accuracy
best_params = current_params.copy()


# Iterate through each hyperparameter and find the best value
for param_name, param_values in hyperparameters.items():
    print(f"\nTuning '{param_name}'...")
    original_value = current_params[param_name]

    for value in param_values:
        current_params[param_name] = value
        dt = DecisionTreeClassifier(random_state=42, **current_params)
        scores = cross_val_score(dt, X_train, y_train, cv=5, scoring='accuracy')
        mean_accuracy = np.mean(scores)

        print(f"  Testing {param_name}={value}: Mean CV Accuracy = {mean_accuracy:.4f}")

        if mean_accuracy > best_accuracy:
            best_accuracy = mean_accuracy
            best_params = current_params.copy()
            print(f"  New best accuracy found: {best_accuracy:.4f} with parameters: {best_params}")

    # Revert the hyperparameter to the best value found in this iteration
    current_params[param_name] = best_params[param_name]

print("\nGreedy tuning complete.")
print(f"Best accuracy found: {best_accuracy:.4f}")
print("Best parameters found:")
print(best_params)



"""## Report & Analysis

Based on the hyperparameter tuning and model evaluation:

**Which hyperparameters gave the best performance?**

According to the greedy hyperparameter tuning performed using cross-validation, the best parameters found are:
- `criterion`: gini
- `max_depth`: 20
- `min_samples_split`: 2
- `min_samples_leaf`: 1

This combination resulted in the highest mean cross-validation accuracy of **0.5822**.

**Did limiting tree depth improve generalization?**

Based on the initial trials:
- Default parameters (likely higher depth) accuracy: 0.5561
- `max_depth=15` accuracy: 0.5623

In this case, limiting the `max_depth` to 15 in the initial trials showed a slight improvement in accuracy compared to the default parameters. However, the greedy search found that a `max_depth` of 20 with other parameters also contributed to the best accuracy. This suggests that a moderately limited depth might help, but the optimal depth depends on the interaction with other hyperparameters. The default `max_depth` (which is `None`, allowing the tree to grow until leaves are pure or contain less than `min_samples_split` samples) might lead to overfitting on the training data, reducing generalization on unseen test data.

**Which features were most important in classifying wine quality?**

Based on the feature importances from the default Decision Tree model:
- `alcohol`: 0.1476
- `volatile acidity`: 0.1030
- `sulphates`: 0.0999
- `free sulfur dioxide`: 0.0951
- `residual sugar`: 0.0811
- `total sulfur dioxide`: 0.0810
- `density`: 0.0808
- `citric acid`: 0.0796
- `chlorides`: 0.0786
- `fixed acidity`: 0.0765
- `pH`: 0.0756
- `type`: 0.0012

The features with the highest importance scores are **alcohol**, **volatile acidity**, and **sulphates**. This indicates that these chemical properties have the most significant impact on the Decision Tree model's ability to classify wine quality. The `type` of wine (red or white) seems to be the least important feature in this model.

**Was the model overfitting/underfitting?**

The accuracy on the test set for the default model was 0.5561, while the mean cross-validation accuracy on the training data for the default parameters was 0.5789. The difference between the training accuracy (from CV) and the test accuracy is relatively small, which might suggest that the model is not severely overfitting.

However, the overall accuracy is still quite low (around 56-58%), and the classification report shows poor performance for some quality classes (e.g., precision and recall of 0 for quality 3 and 9). This indicates that the model might be **underfitting** or that the dataset has inherent characteristics (like class imbalance or features that are not strongly predictive of quality in a linear or simple tree-based manner) that limit the performance of a single Decision Tree model. The low accuracy could also be due to the complexity of the multi-class classification problem with quality ratings ranging from 3 to 9. Further analysis and potentially different modeling techniques might be needed to improve performance.
"""