# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U_g8rGmm7qQd_7LN_D5AaOa4Lfsv4mpR
"""



# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy matplotlib scikit-learn seaborn

import pandas as pd

df = pd.read_csv('heart.csv')

duplicate_rows = df[df.duplicated()]

print("Duplicate Rows:")
display(duplicate_rows)

print(f"\nNumber of duplicate rows: {duplicate_rows.shape[0]}")

# Step 1: Identify Numerical Features
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns
print("Numerical features:", numerical_features)

find_na = df.isna()
print(find_na)

# Find the total number of missing values in each column
missing_values = df.isnull().sum()

# Display the missing values
print("Total missing values per column:")
display(missing_values)



# Step 1: Select Columns
columns_to_scale = ['age', 'trestbps', 'cp', 'restecg', 'thalach', 'oldpeak', 'slope', 'ca', 'thal']
print("Columns to be scaled:", columns_to_scale)

df

# Step 2 & 3: Choose and Initialize Scaler and Scale the Data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Apply the scaler to the selected columns
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

df

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Get correlations with the target variable
target_correlation = correlation_matrix['target'].sort_values(ascending=False)

# Display the correlations
print("Correlation with the target variable:")
print(target_correlation)

# Generate a heatmap of the correlations
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap of Heart Disease Dataset')
plt.show()

from sklearn.model_selection import train_test_split

X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)

# Define the range of hyperparameters to search over
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

print("Hyperparameter grid:", param_grid)

from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')
print("GridSearchCV initialized.")

grid_search.fit(X_train, y_train)
print("GridSearchCV fitting complete.")

"""## Evaluate model

### Subtask:
Evaluate the performance of the best model found by GridSearchCV on the testing set.

**Reasoning**:
Evaluate the performance of the best model on the testing set by making predictions and calculating accuracy.
"""

from sklearn.metrics import accuracy_score

# Get the best model from GridSearchCV
best_model = grid_search.best_estimator_

# Make predictions on the testing set
y_pred = best_model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print(f"Accuracy of the best model on the testing set: {accuracy}")

# Access the best hyperparameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Print the results
print("Best Hyperparameters found by GridSearchCV:")
print(best_params)
print("\nBest Cross-Validation Score:")
print(best_score)

from sklearn.ensemble import RandomForestClassifier

# Instantiate the RandomForestClassifier model with the best hyperparameters
best_params = {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}
model = RandomForestClassifier(**best_params, random_state=42)

# Train the model
model.fit(X_train, y_train)

print("Random Forest model trained with best hyperparameters.")

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

print("Cross-validation scores:", cv_scores)
print(f"Mean cross-validation accuracy: {cv_scores.mean():.4f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Calculate classification metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)

# Print the metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"ROC-AUC: {roc_auc:.4f}")

"""Business Observation -

The target (heart disease) is highly correlated with chest pain type (cp) and ST depression induced by exercise relative to rest (oldpeak), and highly inversely proportional with exercise induced angina (exang) and ST depression induced by exercise relative to rest (oldpeak).
"""